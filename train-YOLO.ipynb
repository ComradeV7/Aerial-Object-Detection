{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c212b23-5214-483e-83c0-804ee0c3b916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\comra\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Torch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "Project Directory: C:\\Users\\comra\\Dev\\Aerial-Object-Detection\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# YOLOv8 uses the 'cuda' device automatically if available\n",
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "print(f\"Project Directory: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e48129-f90a-4fd3-a1fd-3b64b60598a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Configuration File at: C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\data.yaml\n",
      "Content:\n",
      "names:\n",
      "- Bird\n",
      "- Drone\n",
      "nc: 2\n",
      "path: C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\data\\object_detection_Dataset\n",
      "test: test/images\n",
      "train: train/images\n",
      "val: valid/images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Dataset Paths\n",
    "dataset_path = os.path.join(BASE_DIR, 'data\\object_detection_Dataset') \n",
    "\n",
    "# Define the dictionary structure for YOLO\n",
    "data_config = {\n",
    "    'path': dataset_path,          # Root dir\n",
    "    'train': 'train/images',       # Train images (relative to 'path')\n",
    "    'val': 'valid/images',         # Validation images (relative to 'path')\n",
    "    'test': 'test/images',         # Test images (optional)\n",
    "    \n",
    "    # Class Names\n",
    "    'nc': 2,                       # Number of Classes\n",
    "    'names': ['Bird', 'Drone']     # Class names \n",
    "}\n",
    "\n",
    "# Write this dictionary to a .yaml file\n",
    "yaml_path = os.path.join(BASE_DIR, 'data.yaml')\n",
    "\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"Created Configuration File at: {yaml_path}\")\n",
    "print(\"Content:\")\n",
    "print(yaml.dump(data_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259eed09-c029-4cbc-807a-0f1642a2fdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 1.2MB/s 5.3s5.3s<0.0ssss4s\n",
      "Model Loaded: YOLOv8n\n",
      "Ultralytics 8.3.229  Python-3.11.14 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=drone_bird_yolo, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\runs\\detect\\drone_bird_yolo, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to 'C:\\Users\\comra\\AppData\\Roaming\\Ultralytics\\Arial.ttf': 100% ━━━━━━━━━━━━ 755.1KB 1.2MB/s 0.6s 0.6s<0.1s\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ━━━━━━━━━━━━ 5.4MB 1.2MB/s 4.4s 4.4s<0.6sss.3s\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.50.3 ms, read: 2.01.3 MB/s, size: 23.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\data\\object_detection_Dataset\\train\\labels... 2728 images, 66 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 2728/2728 69.1it/s 39.5s<0.1ss\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\data\\object_detection_Dataset\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 5.110.6 ms, read: 1.00.6 MB/s, size: 31.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\data\\object_detection_Dataset\\valid\\labels... 448 images, 6 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 448/448 51.0it/s 8.8s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\data\\object_detection_Dataset\\valid\\labels.cache\n",
      "Plotting labels to C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\runs\\detect\\drone_bird_yolo\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\comra\\Dev\\Aerial-Object-Detection\\runs\\detect\\drone_bird_yolo\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/20       2.1G      1.416      2.369      1.591         21        640: 100% ━━━━━━━━━━━━ 171/171 2.0it/s 1:240.6sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.3it/s 11.1s0.7s\n",
      "                   all        448        663       0.41      0.382      0.339      0.148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/20      2.53G      1.562      2.107      1.713         16        640: 100% ━━━━━━━━━━━━ 171/171 2.1it/s 1:230.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.5it/s 9.1s0.6s\n",
      "                   all        448        663      0.394        0.4      0.354      0.168\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/20      2.53G      1.606      2.006      1.759         15        640: 100% ━━━━━━━━━━━━ 171/171 2.3it/s 1:150.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.6it/s 8.7s0.6s\n",
      "                   all        448        663       0.45      0.466      0.414      0.198\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/20      2.53G      1.605      1.883       1.75         22        640: 100% ━━━━━━━━━━━━ 171/171 2.3it/s 1:160.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.5it/s 9.1s0.6s\n",
      "                   all        448        663      0.505      0.365       0.36      0.152\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/20      2.53G      1.528      1.733       1.69         23        640: 100% ━━━━━━━━━━━━ 171/171 2.4it/s 1:110.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.7it/s 8.4s0.6s\n",
      "                   all        448        663      0.703      0.558      0.598       0.31\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/20      2.53G       1.49      1.636      1.661         32        640: 100% ━━━━━━━━━━━━ 171/171 2.4it/s 1:100.4s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.7it/s 8.3s0.6s\n",
      "                   all        448        663      0.633      0.575      0.594      0.293\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/20      2.53G       1.45      1.568      1.629         14        640: 100% ━━━━━━━━━━━━ 171/171 2.4it/s 1:110.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.7it/s 8.3s0.6s\n",
      "                   all        448        663      0.711      0.636      0.682      0.359\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/20      2.53G      1.406      1.473      1.595         34        640: 100% ━━━━━━━━━━━━ 171/171 2.2it/s 1:180.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.6it/s 8.7s0.6s\n",
      "                   all        448        663      0.778      0.607      0.693      0.381\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/20      2.53G       1.38      1.406      1.566         20        640: 100% ━━━━━━━━━━━━ 171/171 2.2it/s 1:170.5sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.6it/s 8.5s0.6s\n",
      "                   all        448        663      0.753      0.651      0.712      0.395\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/20      2.53G      1.356      1.372      1.545         22        640: 100% ━━━━━━━━━━━━ 171/171 2.2it/s 1:180.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.7it/s 8.5s0.6s\n",
      "                   all        448        663      0.748      0.637      0.692      0.392\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/20      2.53G      1.326      1.233      1.601          8        640: 100% ━━━━━━━━━━━━ 171/171 2.2it/s 1:180.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.6it/s 8.6s0.6s\n",
      "                   all        448        663      0.791      0.638      0.718      0.427\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/20      2.53G      1.286      1.157      1.566         13        640: 100% ━━━━━━━━━━━━ 171/171 2.5it/s 1:090.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.7it/s 8.0s0.6s\n",
      "                   all        448        663      0.789      0.665      0.733      0.446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/20      2.53G      1.247      1.064      1.533          9        640: 100% ━━━━━━━━━━━━ 171/171 2.5it/s 1:090.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.8it/s 7.9s0.6s\n",
      "                   all        448        663       0.84      0.664      0.764      0.462\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/20      2.53G      1.182     0.9866      1.479         10        640: 100% ━━━━━━━━━━━━ 171/171 2.5it/s 1:090.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.7it/s 8.2s0.6s\n",
      "                   all        448        663      0.807      0.717      0.783      0.492\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/20      2.53G       1.16     0.9384      1.453          8        640: 100% ━━━━━━━━━━━━ 171/171 2.5it/s 1:090.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.7it/s 8.3s0.6s\n",
      "                   all        448        663      0.812      0.743        0.8      0.506\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/20      2.53G      1.111     0.8579      1.419         13        640: 100% ━━━━━━━━━━━━ 171/171 2.5it/s 1:090.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.8it/s 8.0s0.6s\n",
      "                   all        448        663      0.795      0.692      0.781      0.489\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/20      2.53G      1.064     0.8178       1.38         10        640: 100% ━━━━━━━━━━━━ 171/171 2.4it/s 1:120.5sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.6it/s 8.7s0.6s\n",
      "                   all        448        663      0.808      0.727      0.795      0.512\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/20      2.53G      1.036     0.7558      1.347         14        640: 100% ━━━━━━━━━━━━ 171/171 2.5it/s 1:090.4s6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.7it/s 8.2s0.6s\n",
      "                   all        448        663      0.883      0.718      0.812      0.526\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/20      2.53G      1.005     0.7222      1.326         11        640: 100% ━━━━━━━━━━━━ 171/171 2.5it/s 1:090.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.8it/s 7.8s0.6s\n",
      "                   all        448        663       0.83      0.734      0.808      0.522\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/20      2.53G     0.9725     0.6997      1.307         11        640: 100% ━━━━━━━━━━━━ 171/171 2.5it/s 1:090.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.7it/s 8.2s0.6s\n",
      "                   all        448        663      0.871      0.752      0.821      0.534\n",
      "\n",
      "20 epochs completed in 0.484 hours.\n",
      "Optimizer stripped from C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\runs\\detect\\drone_bird_yolo\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\runs\\detect\\drone_bird_yolo\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\runs\\detect\\drone_bird_yolo\\weights\\best.pt...\n",
      "Ultralytics 8.3.229  Python-3.11.14 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU, 6144MiB)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 14/14 1.2it/s 11.4s0.7s\n",
      "                   all        448        663      0.869      0.751      0.821      0.534\n",
      "                  Bird        217        414      0.833      0.627      0.737      0.431\n",
      "                 Drone        225        249      0.905      0.876      0.905      0.638\n",
      "Speed: 0.4ms preprocess, 3.3ms inference, 0.0ms loss, 5.2ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\comra\\Dev\\Aerial-Object-Detection\\runs\\detect\\drone_bird_yolo\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model\n",
    "# Load a pre-trained YOLOv8n model\n",
    "model = YOLO('yolov8n.pt') \n",
    "\n",
    "print(\"Model Loaded: YOLOv8n\")\n",
    "\n",
    "# Start Training\n",
    "# epochs=20: Sufficient for YOLO to learn simple objects like drones\n",
    "# imgsz=640: Standard resolution for YOLO\n",
    "# plots=True: Automatically saves graphs of loss/accuracy\n",
    "results = model.train(\n",
    "    data=yaml_path, \n",
    "    epochs=20, \n",
    "    imgsz=640, \n",
    "    batch=16,\n",
    "    name='drone_bird_yolo', # Name of the save folder\n",
    "    device=0 if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93db4085-b24a-40d7-b3d8-c5e929dae114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.229  Python-3.11.14 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU, 6144MiB)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.30.2 ms, read: 35.223.5 MB/s, size: 24.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\data\\object_detection_Dataset\\valid\\labels.cache... 448 images, 6 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 448/448  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 28/28 2.1it/s 13.6s0.4s\n",
      "                   all        448        663      0.871      0.752      0.821      0.534\n",
      "                  Bird        217        414      0.833      0.628      0.737      0.431\n",
      "                 Drone        225        249      0.909      0.876      0.905      0.637\n",
      "Speed: 4.6ms preprocess, 6.6ms inference, 0.0ms loss, 6.1ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\comra\\Dev\\Aerial-Object-Detection\\runs\\detect\\val\u001b[0m\n",
      "mAP50 (Accuracy at 50% overlap): 0.8209\n",
      "mAP50-95 (Strict Accuracy):       0.5342\n"
     ]
    }
   ],
   "source": [
    "# Validate the Best Model\n",
    "# YOLO automatically saves the best model to 'runs/detect/drone_bird_yolo/weights/best.pt'\n",
    "best_model_path = os.path.join(BASE_DIR, 'runs', 'detect', 'drone_bird_yolo', 'weights', 'best.pt')\n",
    "\n",
    "# Load the trained model\n",
    "best_model = YOLO(best_model_path)\n",
    "\n",
    "# Run validation metrics\n",
    "metrics = best_model.val()\n",
    "\n",
    "print(f\"mAP50 (Accuracy at 50% overlap): {metrics.box.map50:.4f}\")\n",
    "print(f\"mAP50-95 (Strict Accuracy):       {metrics.box.map:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46891574-7abe-41c4-abed-5ee21d5bb39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Inference on Test Data...\n",
      "\n",
      "image 1/1 C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\data\\object_detection_Dataset\\test\\images\\104f1bdceabcf7ba_jpg.rf.0b9745f785acd2ddb1dc562b1403efb3.jpg: 640x640 1 Bird, 94.7ms\n",
      "Speed: 11.9ms preprocess, 94.7ms inference, 19.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\data\\object_detection_Dataset\\test\\images\\06b3bc9d00e0bd0c_jpg.rf.db81bbe2cdd9edb36e3405001602520c.jpg: 640x640 1 Bird, 69.3ms\n",
      "Speed: 11.0ms preprocess, 69.3ms inference, 7.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\comra\\Dev\\Aerial-Object-Detection\\data\\object_detection_Dataset\\test\\images\\pic_669_jpg.rf.04ceaf480d5110a1b1a0b625a42d7178.jpg: 640x640 1 Bird, 1 Drone, 38.0ms\n",
      "Speed: 12.3ms preprocess, 38.0ms inference, 11.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "# Run Inference on Test Images\n",
    "test_images_path = os.path.join(dataset_path, 'test', 'images', '*.jpg')\n",
    "test_files = glob.glob(test_images_path)\n",
    "\n",
    "# Pick 3 random images\n",
    "random_files = random.sample(test_files, 3)\n",
    "\n",
    "print(\"Running Inference on Test Data...\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, file in enumerate(random_files):\n",
    "    # Run prediction\n",
    "    # conf=0.5 means \"Only show boxes if you are 50% sure\"\n",
    "    results = best_model.predict(source=file, conf=0.5, save=False)\n",
    "    \n",
    "    # Plot the result\n",
    "    res_plotted = results[0].plot() # Draws boxes on the image\n",
    "    \n",
    "    # Convert Color (OpenCV is BGR, Matplotlib is RGB)\n",
    "    res_rgb = cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.imshow(res_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Test Image {i+1}\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-env",
   "language": "python",
   "name": "py-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
